{
 "cells": [
  {
   "cell_type": "code",
   "id": "5b01c89d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:57:48.032540300Z",
     "start_time": "2025-12-10T03:55:26.215680Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cls_global import gen_khmer_text_image\n",
    "\n",
    "# ============================================\n",
    "# PART 1: Load data from text_process.text\n",
    "# ============================================\n",
    "print(\"Loading data from text_process.text...\")\n",
    "try:\n",
    "    with open(\"datasets/all_cleaned_words.txt\", 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split content into lines\n",
    "    lines = content.strip().split('\\n')\n",
    "    data = pd.DataFrame(lines, columns=['word'])\n",
    "    data = data[data['word'].str.strip() != '']  # Remove empty lines\n",
    "    data['category'] = \"Text Process\"\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from text_process.text\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: text_process.text not found!\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading text_process.text: {e}\")\n",
    "    exit()\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Data Summary ===\")\n",
    "print(f\"Total Data: {len(data)}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 2: Font and Background Variants\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Automatically load all .ttf fonts from the fonts folder\n",
    "fonts_folder = \"fonts\"\n",
    "fonts = glob.glob(os.path.join(fonts_folder, \"*.ttf\"))\n",
    "\n",
    "if len(fonts) == 0:\n",
    "    print(f\"Warning: No .ttf fonts found in '{fonts_folder}' folder!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n=== Fonts Loaded ===\")\n",
    "print(f\"Found {len(fonts)} fonts:\")\n",
    "for font in fonts:\n",
    "    print(f\"  - {os.path.basename(font)}\")\n",
    "\n",
    "font_sizes = [12, 16]\n",
    "\n",
    "bg_colors = [\n",
    "    (255, 255, 255, 255), \n",
    "]\n",
    "\n",
    "noise_levels = [\"low\",\"none\"]\n",
    "blur_levels = [0]\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Train/Valid/Test Split\n",
    "# ============================================\n",
    "if len(data) > 0:\n",
    "    train_valid, test = train_test_split(\n",
    "        data, \n",
    "        test_size=0.2, \n",
    "        stratify=data[\"category\"], \n",
    "        random_state=42\n",
    "    )\n",
    "    train, valid = train_test_split(\n",
    "        train_valid, \n",
    "        test_size=0.1, \n",
    "        stratify=train_valid[\"category\"], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Split Summary ===\")\n",
    "    print(f\"Train: {len(train)}\")\n",
    "    print(f\"Valid: {len(valid)}\")\n",
    "    print(f\"Test: {len(test)}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # PART 4: Generate Images\n",
    "    # ============================================\n",
    "    data_folder = \"data_v1\"\n",
    "    \n",
    "    # Generate training images\n",
    "    print(\"\\n=== Generating Training Images ===\")\n",
    "    for i, (index, row) in enumerate(train.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"train\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(train):\n",
    "            print(f\"{i} of {len(train)}: complete\")\n",
    "    \n",
    "    # Generate validation images\n",
    "    print(\"\\n=== Generating Validation Images ===\")\n",
    "    for i, (index, row) in enumerate(valid.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"valid\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(valid):\n",
    "            print(f\"{i} of {len(valid)}: complete\")\n",
    "    \n",
    "    # Generate testing images\n",
    "    print(\"\\n=== Generating Testing Images ===\")\n",
    "    for i, (index, row) in enumerate(test.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"test\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(test):\n",
    "            print(f\"{i} of {len(test)}: complete\")\n",
    "    \n",
    "    print(\"\\n=== Image Generation Complete ===\")\n",
    "    \n",
    "    # ============================================\n",
    "    # PART 5: Save Train/Valid/Test Labels\n",
    "    # ============================================\n",
    "    print(\"\\n=== Saving Label Files ===\")\n",
    "    \n",
    "    # Save train labels\n",
    "    train_labels = []\n",
    "    for index, row in train.iterrows():\n",
    "        train_labels.append(f\"train/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/train.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_labels))\n",
    "    print(f\"Saved {len(train_labels)} training labels to {data_folder}/train.txt\")\n",
    "    \n",
    "    # Save valid labels\n",
    "    valid_labels = []\n",
    "    for index, row in valid.iterrows():\n",
    "        valid_labels.append(f\"valid/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/valid.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(valid_labels))\n",
    "    print(f\"Saved {len(valid_labels)} validation labels to {data_folder}/valid.txt\")\n",
    "    \n",
    "    # Save test labels\n",
    "    test_labels = []\n",
    "    for index, row in test.iterrows():\n",
    "        test_labels.append(f\"test/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/test.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(test_labels))\n",
    "    print(f\"Saved {len(test_labels)} test labels to {data_folder}/test.txt\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for splitting and image generation.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from text_process.text...\n",
      "Loaded 127491 entries from text_process.text\n",
      "\n",
      "=== Data Summary ===\n",
      "Total Data: 127491\n",
      "\n",
      "=== Fonts Loaded ===\n",
      "Found 15 fonts:\n",
      "  - KhmerDigital-Black.ttf\n",
      "  - KhmerDigital-Bold.ttf\n",
      "  - KhmerDigital-ExtraBold.ttf\n",
      "  - KhmerDigital-ExtraLight.ttf\n",
      "  - KhmerDigital-Light.ttf\n",
      "  - KhmerDigital-Medium.ttf\n",
      "  - KhmerDigital-Regular.ttf\n",
      "  - KhmerDigital-SemiBold.ttf\n",
      "  - KhmerDigital-Thin.ttf\n",
      "  - KhmerDigitalMax.ttf\n",
      "  - KhmerDigitalNumber.ttf\n",
      "  - KhmerDigitalNumberMax.ttf\n",
      "  - KhmerMPTC.ttf\n",
      "  - KhmerOS_muollight.ttf\n",
      "  - KhmerOS_siemreap.ttf\n",
      "\n",
      "=== Split Summary ===\n",
      "Train: 91792\n",
      "Valid: 10200\n",
      "Test: 25499\n",
      "\n",
      "=== Generating Training Images ===\n",
      "100 of 91792: complete\n",
      "200 of 91792: complete\n",
      "300 of 91792: complete\n",
      "400 of 91792: complete\n",
      "500 of 91792: complete\n",
      "600 of 91792: complete\n",
      "700 of 91792: complete\n",
      "800 of 91792: complete\n",
      "900 of 91792: complete\n",
      "1000 of 91792: complete\n",
      "1100 of 91792: complete\n",
      "1200 of 91792: complete\n",
      "1300 of 91792: complete\n",
      "1400 of 91792: complete\n",
      "1500 of 91792: complete\n",
      "1600 of 91792: complete\n",
      "1700 of 91792: complete\n",
      "1800 of 91792: complete\n",
      "1900 of 91792: complete\n",
      "2000 of 91792: complete\n",
      "2100 of 91792: complete\n",
      "2200 of 91792: complete\n",
      "2300 of 91792: complete\n",
      "2400 of 91792: complete\n",
      "2500 of 91792: complete\n",
      "2600 of 91792: complete\n",
      "2700 of 91792: complete\n",
      "2800 of 91792: complete\n",
      "2900 of 91792: complete\n",
      "3000 of 91792: complete\n",
      "3100 of 91792: complete\n",
      "3200 of 91792: complete\n",
      "3300 of 91792: complete\n",
      "3400 of 91792: complete\n",
      "3500 of 91792: complete\n",
      "3600 of 91792: complete\n",
      "3700 of 91792: complete\n",
      "3800 of 91792: complete\n",
      "3900 of 91792: complete\n",
      "4000 of 91792: complete\n",
      "4100 of 91792: complete\n",
      "4200 of 91792: complete\n",
      "4300 of 91792: complete\n",
      "4400 of 91792: complete\n",
      "4500 of 91792: complete\n",
      "4600 of 91792: complete\n",
      "4700 of 91792: complete\n",
      "4800 of 91792: complete\n",
      "4900 of 91792: complete\n",
      "5000 of 91792: complete\n",
      "5100 of 91792: complete\n",
      "5200 of 91792: complete\n",
      "5300 of 91792: complete\n",
      "5400 of 91792: complete\n",
      "5500 of 91792: complete\n",
      "5600 of 91792: complete\n",
      "5700 of 91792: complete\n",
      "5800 of 91792: complete\n",
      "5900 of 91792: complete\n",
      "6000 of 91792: complete\n",
      "6100 of 91792: complete\n",
      "6200 of 91792: complete\n",
      "6300 of 91792: complete\n",
      "6400 of 91792: complete\n",
      "6500 of 91792: complete\n",
      "6600 of 91792: complete\n",
      "6700 of 91792: complete\n",
      "6800 of 91792: complete\n",
      "6900 of 91792: complete\n",
      "7000 of 91792: complete\n",
      "7100 of 91792: complete\n",
      "7200 of 91792: complete\n",
      "7300 of 91792: complete\n",
      "7400 of 91792: complete\n",
      "7500 of 91792: complete\n",
      "7600 of 91792: complete\n",
      "7700 of 91792: complete\n",
      "7800 of 91792: complete\n",
      "7900 of 91792: complete\n",
      "8000 of 91792: complete\n",
      "8100 of 91792: complete\n",
      "8200 of 91792: complete\n",
      "8300 of 91792: complete\n",
      "8400 of 91792: complete\n",
      "8500 of 91792: complete\n",
      "8600 of 91792: complete\n",
      "8700 of 91792: complete\n",
      "8800 of 91792: complete\n",
      "8900 of 91792: complete\n",
      "9000 of 91792: complete\n",
      "9100 of 91792: complete\n",
      "9200 of 91792: complete\n",
      "9300 of 91792: complete\n",
      "9400 of 91792: complete\n",
      "9500 of 91792: complete\n",
      "9600 of 91792: complete\n",
      "9700 of 91792: complete\n",
      "9800 of 91792: complete\n",
      "9900 of 91792: complete\n",
      "10000 of 91792: complete\n",
      "10100 of 91792: complete\n",
      "10200 of 91792: complete\n",
      "10300 of 91792: complete\n",
      "10400 of 91792: complete\n",
      "10500 of 91792: complete\n",
      "10600 of 91792: complete\n",
      "10700 of 91792: complete\n",
      "10800 of 91792: complete\n",
      "10900 of 91792: complete\n",
      "11000 of 91792: complete\n",
      "11100 of 91792: complete\n",
      "11200 of 91792: complete\n",
      "11300 of 91792: complete\n",
      "11400 of 91792: complete\n",
      "11500 of 91792: complete\n",
      "11600 of 91792: complete\n",
      "11700 of 91792: complete\n",
      "11800 of 91792: complete\n",
      "11900 of 91792: complete\n",
      "12000 of 91792: complete\n",
      "12100 of 91792: complete\n",
      "12200 of 91792: complete\n",
      "12300 of 91792: complete\n",
      "12400 of 91792: complete\n",
      "12500 of 91792: complete\n",
      "12600 of 91792: complete\n",
      "12700 of 91792: complete\n",
      "12800 of 91792: complete\n",
      "12900 of 91792: complete\n",
      "13000 of 91792: complete\n",
      "13100 of 91792: complete\n",
      "13200 of 91792: complete\n",
      "13300 of 91792: complete\n",
      "13400 of 91792: complete\n",
      "13500 of 91792: complete\n",
      "13600 of 91792: complete\n",
      "13700 of 91792: complete\n",
      "13800 of 91792: complete\n",
      "13900 of 91792: complete\n",
      "14000 of 91792: complete\n",
      "14100 of 91792: complete\n",
      "14200 of 91792: complete\n",
      "14300 of 91792: complete\n",
      "14400 of 91792: complete\n",
      "14500 of 91792: complete\n",
      "14600 of 91792: complete\n",
      "14700 of 91792: complete\n",
      "14800 of 91792: complete\n",
      "14900 of 91792: complete\n",
      "15000 of 91792: complete\n",
      "15100 of 91792: complete\n",
      "15200 of 91792: complete\n",
      "15300 of 91792: complete\n",
      "15400 of 91792: complete\n",
      "15500 of 91792: complete\n",
      "15600 of 91792: complete\n",
      "15700 of 91792: complete\n",
      "15800 of 91792: complete\n",
      "15900 of 91792: complete\n",
      "16000 of 91792: complete\n",
      "16100 of 91792: complete\n",
      "16200 of 91792: complete\n",
      "16300 of 91792: complete\n",
      "16400 of 91792: complete\n",
      "16500 of 91792: complete\n",
      "16600 of 91792: complete\n",
      "16700 of 91792: complete\n",
      "16800 of 91792: complete\n",
      "16900 of 91792: complete\n",
      "17000 of 91792: complete\n",
      "17100 of 91792: complete\n",
      "17200 of 91792: complete\n",
      "17300 of 91792: complete\n",
      "17400 of 91792: complete\n",
      "17500 of 91792: complete\n",
      "17600 of 91792: complete\n",
      "17700 of 91792: complete\n",
      "17800 of 91792: complete\n",
      "17900 of 91792: complete\n",
      "18000 of 91792: complete\n",
      "18100 of 91792: complete\n",
      "18200 of 91792: complete\n",
      "18300 of 91792: complete\n",
      "18400 of 91792: complete\n",
      "18500 of 91792: complete\n",
      "18600 of 91792: complete\n",
      "18700 of 91792: complete\n",
      "18800 of 91792: complete\n",
      "18900 of 91792: complete\n",
      "19000 of 91792: complete\n",
      "19100 of 91792: complete\n",
      "19200 of 91792: complete\n",
      "19300 of 91792: complete\n",
      "19400 of 91792: complete\n",
      "19500 of 91792: complete\n",
      "19600 of 91792: complete\n",
      "19700 of 91792: complete\n",
      "19800 of 91792: complete\n",
      "19900 of 91792: complete\n",
      "20000 of 91792: complete\n",
      "20100 of 91792: complete\n",
      "20200 of 91792: complete\n",
      "20300 of 91792: complete\n",
      "20400 of 91792: complete\n",
      "20500 of 91792: complete\n",
      "20600 of 91792: complete\n",
      "20700 of 91792: complete\n",
      "20800 of 91792: complete\n",
      "20900 of 91792: complete\n",
      "21000 of 91792: complete\n",
      "21100 of 91792: complete\n",
      "21200 of 91792: complete\n",
      "21300 of 91792: complete\n",
      "21400 of 91792: complete\n",
      "21500 of 91792: complete\n",
      "21600 of 91792: complete\n",
      "21700 of 91792: complete\n",
      "21800 of 91792: complete\n",
      "21900 of 91792: complete\n",
      "22000 of 91792: complete\n",
      "22100 of 91792: complete\n",
      "22200 of 91792: complete\n",
      "22300 of 91792: complete\n",
      "22400 of 91792: complete\n",
      "22500 of 91792: complete\n",
      "22600 of 91792: complete\n",
      "22700 of 91792: complete\n",
      "22800 of 91792: complete\n",
      "22900 of 91792: complete\n",
      "23000 of 91792: complete\n",
      "23100 of 91792: complete\n",
      "23200 of 91792: complete\n",
      "23300 of 91792: complete\n",
      "23400 of 91792: complete\n",
      "23500 of 91792: complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 98\u001B[39m\n\u001B[32m     95\u001B[39m noise_level = random.choice(noise_levels)\n\u001B[32m     96\u001B[39m blur_level = random.choice(blur_levels)\n\u001B[32m---> \u001B[39m\u001B[32m98\u001B[39m \u001B[43mgen_khmer_text_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     99\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m+\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    100\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mword\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbg\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnoise_level\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnoise_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m    \u001B[49m\u001B[43mblur_level\u001B[49m\u001B[43m=\u001B[49m\u001B[43mblur_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfont_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfont\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfont_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfont_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_folder\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m i % \u001B[32m100\u001B[39m == \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i == \u001B[38;5;28mlen\u001B[39m(train):\n\u001B[32m    110\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: complete\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - MPTC\\Documents\\AI_MPTC\\Khmer_text_recognition\\validation_model\\cls_global.py:85\u001B[39m, in \u001B[36mgen_khmer_text_image\u001B[39m\u001B[34m(index, content, data_type, bg, noise_level, blur_level, font_path, font_size, data_folder)\u001B[39m\n\u001B[32m     82\u001B[39m text_position = (text_x, text_y)\n\u001B[32m     84\u001B[39m \u001B[38;5;66;03m# Draw text in black\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[43mdraw\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_position\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfont\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfont\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m255\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     87\u001B[39m \u001B[38;5;66;03m# Convert to RGB\u001B[39;00m\n\u001B[32m     88\u001B[39m image = image.convert(\u001B[33m'\u001B[39m\u001B[33mRGB\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\ImageDraw.py:565\u001B[39m, in \u001B[36mImageDraw.text\u001B[39m\u001B[34m(self, xy, text, fill, font, anchor, spacing, align, direction, features, language, stroke_width, stroke_fill, embedded_color, *args, **kwargs)\u001B[39m\n\u001B[32m    562\u001B[39m     draw_text(ink, \u001B[32m0\u001B[39m)\n\u001B[32m    563\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    564\u001B[39m     \u001B[38;5;66;03m# Only draw normal text\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m565\u001B[39m     \u001B[43mdraw_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mink\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\ImageDraw.py:508\u001B[39m, in \u001B[36mImageDraw.text.<locals>.draw_text\u001B[39m\u001B[34m(ink, stroke_width, stroke_offset)\u001B[39m\n\u001B[32m    506\u001B[39m     start.append(math.modf(xy[i])[\u001B[32m0\u001B[39m])\n\u001B[32m    507\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m508\u001B[39m     mask, offset = \u001B[43mfont\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetmask2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    509\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    510\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    511\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    512\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    513\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    514\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstroke_width\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstroke_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    515\u001B[39m \u001B[43m        \u001B[49m\u001B[43manchor\u001B[49m\u001B[43m=\u001B[49m\u001B[43manchor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    516\u001B[39m \u001B[43m        \u001B[49m\u001B[43mink\u001B[49m\u001B[43m=\u001B[49m\u001B[43mink\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    517\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    521\u001B[39m     coord = coord[\u001B[32m0\u001B[39m] + offset[\u001B[32m0\u001B[39m], coord[\u001B[32m1\u001B[39m] + offset[\u001B[32m1\u001B[39m]\n\u001B[32m    522\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from jiwer import cer, wer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose, RandomRotation, ToPILImage\n",
    "import shutil\n",
    "\n",
    "# ============================================\n",
    "# PART 1: Dataset Class\n",
    "# ============================================\n",
    "class KhmerTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, processor, transform=None, max_target_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        # Tokenize label\n",
    "        labels = self.processor.tokenizer(\n",
    "            label,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"pixel_values\": image, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "# ============================================\n",
    "# PART 2: Helper Functions\n",
    "# ============================================\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load tab-separated data\"\"\"\n",
    "    data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"image\", \"label\"])\n",
    "    return data\n",
    "\n",
    "def create_dataloader(data, root_dir, processor, batch_size=16, shuffle=True, max_length=128, transform=None, data_type=\"train\"):\n",
    "    \"\"\"Create DataLoader from dataset\"\"\"\n",
    "    # Add data_type subdirectory to root_dir\n",
    "    full_root_dir = os.path.join(root_dir, data_type)\n",
    "    dataset = KhmerTextDataset(data, full_root_dir, processor, max_target_length=max_length, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def save_checkpoint(model, processor, optimizer, epoch, step, checkpoint_dir=\"checkpoint_latest\"):\n",
    "    \"\"\"Save latest checkpoint (overwrites previous)\"\"\"\n",
    "    # Remove old checkpoint if exists\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir)\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model and processor\n",
    "    model.save_pretrained(checkpoint_dir)\n",
    "    processor.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # Save optimizer state and training info\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "    \n",
    "    print(f\"\\n✓ Checkpoint saved: Epoch {epoch}, Step {step}\")\n",
    "    \n",
    "    # Auto-download if running in Google Colab\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        import zipfile\n",
    "        \n",
    "        # Create zip file\n",
    "        zip_filename = f\"checkpoint_epoch{epoch}_step{step}.zip\"\n",
    "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files_list in os.walk(checkpoint_dir):\n",
    "                for file in files_list:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, checkpoint_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        print(f\"✓ Checkpoint zipped: {zip_filename}\")\n",
    "        print(f\"⬇ Downloading checkpoint...\")\n",
    "        files.download(zip_filename)\n",
    "        \n",
    "        # Clean up zip file after download\n",
    "        os.remove(zip_filename)\n",
    "        print(f\"✓ Download complete!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        # Not in Colab, skip download\n",
    "        print(f\"✓ Checkpoint saved to: {checkpoint_dir}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Configuration\n",
    "# ============================================\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "data_path = \"data_v1\"\n",
    "epochs = 20\n",
    "checkpoint_dir = \"checkpoint_latest\"  # Single checkpoint folder\n",
    "\n",
    "# ============================================\n",
    "# PART 4: Load Datasets\n",
    "# ============================================\n",
    "print(\"Loading datasets...\")\n",
    "train_data = load_dataset(f\"{data_path}/train.txt\")\n",
    "valid_data = load_dataset(f\"{data_path}/valid.txt\")\n",
    "test_data = load_dataset(f\"{data_path}/test.txt\")\n",
    "\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Train: {len(train_data)} samples\")\n",
    "print(f\"Valid: {len(valid_data)} samples\")\n",
    "print(f\"Test: {len(test_data)} samples\")\n",
    "print(\"\\nSample train data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# ============================================\n",
    "# PART 5: Load Model and Processor\n",
    "# ============================================\n",
    "print(\"\\n=== Loading TrOCR Model ===\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 6: Create DataLoaders\n",
    "# ============================================\n",
    "print(\"\\n=== Creating DataLoaders ===\")\n",
    "transform = Compose([\n",
    "    Resize((384, 384)),  # Resize to match ViT input size\n",
    "    RandomRotation(degrees=5),  # Add slight rotation\n",
    "    ToTensor(),  # Convert to PyTorch Tensor\n",
    "    Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_data, f\"{data_path}/\", processor,\n",
    "    batch_size=batch_size, max_length=max_length, transform=transform, data_type=\"train\"\n",
    ")\n",
    "valid_loader = create_dataloader(\n",
    "    valid_data, f\"{data_path}/\", processor,\n",
    "    batch_size=batch_size, max_length=max_length, transform=transform, data_type=\"valid\"\n",
    ")\n",
    "test_loader = create_dataloader(\n",
    "    test_data, f\"{data_path}/\", processor,\n",
    "    batch_size=batch_size, max_length=max_length, transform=transform, data_type=\"test\"\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Valid batches: {len(valid_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 7: Visualize Sample Batch\n",
    "# ============================================\n",
    "print(\"\\n=== Visualizing Sample Batch ===\")\n",
    "reverse_transform = ToPILImage()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(\"Pixel Values Shape:\", batch[\"pixel_values\"].shape)\n",
    "    print(\"Labels Shape:\", batch[\"labels\"].shape)\n",
    "\n",
    "    # Show first image in batch\n",
    "    label = batch[\"labels\"][0]\n",
    "    decoded_label = processor.tokenizer.decode(label.tolist(), skip_special_tokens=True)\n",
    "    print(f\"Decoded Label: {decoded_label}\")\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"][0]\n",
    "    image = reverse_transform(pixel_values)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {decoded_label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    if i == 2:  # Show only first 3 batches\n",
    "        break\n",
    "\n",
    "# ============================================\n",
    "# PART 8: Training Setup\n",
    "# ============================================\n",
    "print(\"\\n=== Training Setup ===\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "cer_scores = []\n",
    "wer_scores = []\n",
    "\n",
    "# Initialize global step counter\n",
    "global_step = 0\n",
    "\n",
    "# ============================================\n",
    "# PART 9: Training Loop with Checkpointing\n",
    "# ============================================\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "print(f\"Checkpoints will be saved and downloaded after each epoch\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs} - Training\")\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment global step\n",
    "        global_step += 1\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Global Step: {global_step}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "        for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels.to(device))\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # Decode predictions and references\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            references = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "\n",
    "    # Calculate CER and WER\n",
    "    cer_score = cer(all_references, all_predictions)\n",
    "    wer_score = wer(all_references, all_predictions)\n",
    "    cer_scores.append(cer_score)\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n",
    "    \n",
    "    # Save checkpoint at the end of each epoch\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Saving checkpoint for Epoch {epoch + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    save_checkpoint(model, processor, optimizer, epoch + 1, global_step, checkpoint_dir)\n",
    "\n",
    "# ============================================\n",
    "# PART 10: Plot Training Results\n",
    "# ============================================\n",
    "print(\"\\n=== Plotting Results ===\")\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "# Training and Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, training_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(epochs_range, validation_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# CER and WER\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, cer_scores, label=\"CER\", marker='o')\n",
    "plt.plot(epochs_range, wer_scores, label=\"WER\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"CER and WER over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PART 11: Save Final Model\n",
    "# ============================================\n",
    "print(\"\\n=== Saving Final Model ===\")\n",
    "model.save_pretrained(\"khmer_text_recognition_model_v3\")\n",
    "processor.save_pretrained(\"khmer_text_recognition_processor_v3\")\n",
    "print(\"Model and processor saved successfully!\")\n",
    "\n",
    "# Auto-download final model\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import zipfile\n",
    "    \n",
    "    # Create zip file for final model\n",
    "    final_zip = \"khmer_text_recognition_model_final.zip\"\n",
    "    with zipfile.ZipFile(final_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files_list in os.walk(\"khmer_text_recognition_model_v3\"):\n",
    "            for file in files_list:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, \"khmer_text_recognition_model_v3\")\n",
    "                zipf.write(file_path, os.path.join(\"model\", arcname))\n",
    "        \n",
    "        for root, dirs, files_list in os.walk(\"khmer_text_recognition_processor_v3\"):\n",
    "            for file in files_list:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, \"khmer_text_recognition_processor_v3\")\n",
    "                zipf.write(file_path, os.path.join(\"processor\", arcname))\n",
    "    \n",
    "    print(f\"✓ Final model zipped: {final_zip}\")\n",
    "    print(f\"⬇ Downloading final model...\")\n",
    "    files.download(final_zip)\n",
    "    print(f\"✓ Final model download complete!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - skipping auto-download\")\n",
    "\n",
    "# ============================================\n",
    "# PART 12: Test the Model\n",
    "# ============================================\n",
    "print(\"\\n=== Testing Model ===\")\n",
    "# Load saved model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"khmer_text_recognition_processor_v3\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"khmer_text_recognition_model_v3\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_preds, test_refs = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(pixel_values, max_new_tokens=128)\n",
    "        decoded_preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        test_preds.extend(decoded_preds)\n",
    "        test_refs.extend(decoded_labels)\n",
    "\n",
    "# Calculate CER and WER\n",
    "test_cer = cer(test_refs, test_preds)\n",
    "test_wer = wer(test_refs, test_preds)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "for i, (pred, ref) in enumerate(zip(test_preds[:10], test_refs[:10])):\n",
    "    print(f\"\\n{i+1}.\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Display overall metrics\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print(f\"Character Error Rate (CER): {test_cer:.4f}\")\n",
    "print(f\"Word Error Rate (WER): {test_wer:.4f}\")\n",
    "print(f\"\\nTotal training steps: {global_step}\")\n",
    "print(f\"Total epochs completed: {epochs}\")\n",
    "print(\"\\n=== Training Complete ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
