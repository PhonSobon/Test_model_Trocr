{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from text_process.text...\n",
      "Error: text_process.text not found!\n",
      "\n",
      "=== Data Summary ===\n",
      "Total Data: 127491\n",
      "\n",
      "=== Fonts Loaded ===\n",
      "Found 15 fonts:\n",
      "  - KhmerDigital-Black.ttf\n",
      "  - KhmerDigital-Bold.ttf\n",
      "  - KhmerDigital-ExtraBold.ttf\n",
      "  - KhmerDigital-ExtraLight.ttf\n",
      "  - KhmerDigital-Light.ttf\n",
      "  - KhmerDigital-Medium.ttf\n",
      "  - KhmerDigital-Regular.ttf\n",
      "  - KhmerDigital-SemiBold.ttf\n",
      "  - KhmerDigital-Thin.ttf\n",
      "  - KhmerDigitalMax.ttf\n",
      "  - KhmerDigitalNumber.ttf\n",
      "  - KhmerDigitalNumberMax.ttf\n",
      "  - KhmerMPTC.ttf\n",
      "  - KhmerOS_muollight.ttf\n",
      "  - KhmerOS_siemreap.ttf\n",
      "\n",
      "=== Split Summary ===\n",
      "Train: 91792\n",
      "Valid: 10200\n",
      "Test: 25499\n",
      "\n",
      "=== Generating Training Images ===\n",
      "100 of 91792: complete\n",
      "200 of 91792: complete\n",
      "300 of 91792: complete\n",
      "400 of 91792: complete\n",
      "500 of 91792: complete\n",
      "600 of 91792: complete\n",
      "700 of 91792: complete\n",
      "800 of 91792: complete\n",
      "900 of 91792: complete\n",
      "1000 of 91792: complete\n",
      "1100 of 91792: complete\n",
      "1200 of 91792: complete\n",
      "1300 of 91792: complete\n",
      "1400 of 91792: complete\n",
      "1500 of 91792: complete\n",
      "1600 of 91792: complete\n",
      "1700 of 91792: complete\n",
      "1800 of 91792: complete\n",
      "1900 of 91792: complete\n",
      "2000 of 91792: complete\n",
      "2100 of 91792: complete\n",
      "2200 of 91792: complete\n",
      "2300 of 91792: complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m noise_level = random.choice(noise_levels)\n\u001b[32m     96\u001b[39m blur_level = random.choice(blur_levels)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mgen_khmer_text_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblur_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblur_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfont_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfont_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_folder\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(train):\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: complete\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PhonSobon\\OneDrive - MPTC\\Documents\\AI_MPTC\\Khmer_text_recognition\\validation_model\\cls_global.py:100\u001b[39m, in \u001b[36mgen_khmer_text_image\u001b[39m\u001b[34m(index, content, data_type, bg, noise_level, blur_level, font_path, font_size, data_folder)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Save image\u001b[39;00m\n\u001b[32m     99\u001b[39m output_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PhonSobon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:2560\u001b[39m, in \u001b[36mImage.save\u001b[39m\u001b[34m(self, fp, format, **params)\u001b[39m\n\u001b[32m   2558\u001b[39m created = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     created = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2561\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params.get(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2562\u001b[39m         \u001b[38;5;66;03m# Open also for reading (\"+\"), because TIFF save_all\u001b[39;00m\n\u001b[32m   2563\u001b[39m         \u001b[38;5;66;03m# writer needs to go back and edit the written data.\u001b[39;00m\n\u001b[32m   2564\u001b[39m         fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mr+b\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cls_global import gen_khmer_text_image\n",
    "\n",
    "# ============================================\n",
    "# PART 1: Load data from text_process.text\n",
    "# ============================================\n",
    "print(\"Loading data from text_process.text...\")\n",
    "try:\n",
    "    with open(\"text_process.text\", 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split content into lines\n",
    "    lines = content.strip().split('\\n')\n",
    "    data = pd.DataFrame(lines, columns=['word'])\n",
    "    data = data[data['word'].str.strip() != '']  # Remove empty lines\n",
    "    data['category'] = \"Text Process\"\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from text_process.text\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: text_process.text not found!\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading text_process.text: {e}\")\n",
    "    exit()\n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Data Summary ===\")\n",
    "print(f\"Total Data: {len(data)}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 2: Font and Background Variants\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Automatically load all .ttf fonts from the fonts folder\n",
    "fonts_folder = \"fonts\"\n",
    "fonts = glob.glob(os.path.join(fonts_folder, \"*.ttf\"))\n",
    "\n",
    "if len(fonts) == 0:\n",
    "    print(f\"Warning: No .ttf fonts found in '{fonts_folder}' folder!\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n=== Fonts Loaded ===\")\n",
    "print(f\"Found {len(fonts)} fonts:\")\n",
    "for font in fonts:\n",
    "    print(f\"  - {os.path.basename(font)}\")\n",
    "\n",
    "font_sizes = [12, 16]\n",
    "\n",
    "bg_colors = [\n",
    "    (255, 255, 255, 255), \n",
    "]\n",
    "\n",
    "noise_levels = [\"low\",\"none\"]\n",
    "blur_levels = [0]\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Train/Valid/Test Split\n",
    "# ============================================\n",
    "if len(data) > 0:\n",
    "    train_valid, test = train_test_split(\n",
    "        data, \n",
    "        test_size=0.2, \n",
    "        stratify=data[\"category\"], \n",
    "        random_state=42\n",
    "    )\n",
    "    train, valid = train_test_split(\n",
    "        train_valid, \n",
    "        test_size=0.1, \n",
    "        stratify=train_valid[\"category\"], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Split Summary ===\")\n",
    "    print(f\"Train: {len(train)}\")\n",
    "    print(f\"Valid: {len(valid)}\")\n",
    "    print(f\"Test: {len(test)}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # PART 4: Generate Images\n",
    "    # ============================================\n",
    "    data_folder = \"data_v1\"\n",
    "    \n",
    "    # Generate training images\n",
    "    print(\"\\n=== Generating Training Images ===\")\n",
    "    for i, (index, row) in enumerate(train.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"train\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(train):\n",
    "            print(f\"{i} of {len(train)}: complete\")\n",
    "    \n",
    "    # Generate validation images\n",
    "    print(\"\\n=== Generating Validation Images ===\")\n",
    "    for i, (index, row) in enumerate(valid.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"valid\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(valid):\n",
    "            print(f\"{i} of {len(valid)}: complete\")\n",
    "    \n",
    "    # Generate testing images\n",
    "    print(\"\\n=== Generating Testing Images ===\")\n",
    "    for i, (index, row) in enumerate(test.iterrows(), 1):\n",
    "        font_size = random.choice(font_sizes)\n",
    "        font = random.choice(fonts)\n",
    "        bg = random.choice(bg_colors)\n",
    "        noise_level = random.choice(noise_levels)\n",
    "        blur_level = random.choice(blur_levels)\n",
    "        \n",
    "        gen_khmer_text_image(\n",
    "            index=index+1, \n",
    "            content=row[\"word\"],\n",
    "            data_type=\"test\", \n",
    "            bg=bg, \n",
    "            noise_level=noise_level, \n",
    "            blur_level=blur_level,\n",
    "            font_path=font, \n",
    "            font_size=font_size,\n",
    "            data_folder=data_folder\n",
    "        )\n",
    "        if i % 100 == 0 or i == len(test):\n",
    "            print(f\"{i} of {len(test)}: complete\")\n",
    "    \n",
    "    print(\"\\n=== Image Generation Complete ===\")\n",
    "    \n",
    "    # ============================================\n",
    "    # PART 5: Save Train/Valid/Test Labels\n",
    "    # ============================================\n",
    "    print(\"\\n=== Saving Label Files ===\")\n",
    "    \n",
    "    # Save train labels\n",
    "    train_labels = []\n",
    "    for index, row in train.iterrows():\n",
    "        train_labels.append(f\"train/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/train.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_labels))\n",
    "    print(f\"Saved {len(train_labels)} training labels to {data_folder}/train.txt\")\n",
    "    \n",
    "    # Save valid labels\n",
    "    valid_labels = []\n",
    "    for index, row in valid.iterrows():\n",
    "        valid_labels.append(f\"valid/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/valid.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(valid_labels))\n",
    "    print(f\"Saved {len(valid_labels)} validation labels to {data_folder}/valid.txt\")\n",
    "    \n",
    "    # Save test labels\n",
    "    test_labels = []\n",
    "    for index, row in test.iterrows():\n",
    "        test_labels.append(f\"test/{index+1}.png\\t{row['word']}\")\n",
    "    with open(f\"{data_folder}/test.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(test_labels))\n",
    "    print(f\"Saved {len(test_labels)} test labels to {data_folder}/test.txt\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for splitting and image generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d082e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from jiwer import cer, wer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose, RandomRotation, ToPILImage\n",
    "\n",
    "# ============================================\n",
    "# PART 1: Dataset Class\n",
    "# ============================================\n",
    "class KhmerTextDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, processor, transform=None, max_target_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        # Tokenize label\n",
    "        labels = self.processor.tokenizer(\n",
    "            label, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_target_length, \n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"pixel_values\": image, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "# ============================================\n",
    "# PART 2: Helper Functions\n",
    "# ============================================\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load tab-separated data\"\"\"\n",
    "    data = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"image\", \"label\"])\n",
    "    return data\n",
    "\n",
    "def create_dataloader(data, root_dir, processor, batch_size=16, shuffle=True, max_length=128, transform=None):\n",
    "    \"\"\"Create DataLoader from dataset\"\"\"\n",
    "    dataset = KhmerTextDataset(data, root_dir, processor, max_target_length=max_length, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# ============================================\n",
    "# PART 3: Configuration\n",
    "# ============================================\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "data_path = \"data_v1\"\n",
    "epochs = 10\n",
    "\n",
    "# ============================================\n",
    "# PART 4: Load Datasets\n",
    "# ============================================\n",
    "print(\"Loading datasets...\")\n",
    "train_data = load_dataset(f\"{data_path}/train.txt\")\n",
    "valid_data = load_dataset(f\"{data_path}/valid.txt\")\n",
    "test_data = load_dataset(f\"{data_path}/test.txt\")\n",
    "\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Train: {len(train_data)} samples\")\n",
    "print(f\"Valid: {len(valid_data)} samples\")\n",
    "print(f\"Test: {len(test_data)} samples\")\n",
    "print(\"\\nSample train data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# ============================================\n",
    "# PART 5: Load Model and Processor\n",
    "# ============================================\n",
    "print(\"\\n=== Loading TrOCR Model ===\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 6: Create DataLoaders\n",
    "# ============================================\n",
    "print(\"\\n=== Creating DataLoaders ===\")\n",
    "transform = Compose([\n",
    "    Resize((384, 384)),  # Resize to match ViT input size\n",
    "    RandomRotation(degrees=5),  # Add slight rotation\n",
    "    ToTensor(),  # Convert to PyTorch Tensor\n",
    "    Normalize(mean=[0.5], std=[0.5])  # Normalize pixel values\n",
    "])\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_data, f\"{data_path}/train/\", processor, \n",
    "    batch_size=batch_size, max_length=max_length, transform=transform\n",
    ")\n",
    "valid_loader = create_dataloader(\n",
    "    valid_data, f\"{data_path}/valid/\", processor, \n",
    "    batch_size=batch_size, max_length=max_length, transform=transform\n",
    ")\n",
    "test_loader = create_dataloader(\n",
    "    test_data, f\"{data_path}/test/\", processor, \n",
    "    batch_size=batch_size, max_length=max_length, transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Valid batches: {len(valid_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 7: Visualize Sample Batch\n",
    "# ============================================\n",
    "print(\"\\n=== Visualizing Sample Batch ===\")\n",
    "reverse_transform = ToPILImage()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"\\nBatch {i + 1}:\")\n",
    "    print(\"Pixel Values Shape:\", batch[\"pixel_values\"].shape)\n",
    "    print(\"Labels Shape:\", batch[\"labels\"].shape)\n",
    "\n",
    "    # Show first image in batch\n",
    "    label = batch[\"labels\"][0]\n",
    "    decoded_label = processor.tokenizer.decode(label.tolist(), skip_special_tokens=True)\n",
    "    print(f\"Decoded Label: {decoded_label}\")\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"][0]\n",
    "    image = reverse_transform(pixel_values)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {decoded_label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    if i == 2:  # Show only first 3 batches\n",
    "        break\n",
    "\n",
    "# ============================================\n",
    "# PART 8: Training Setup\n",
    "# ============================================\n",
    "print(\"\\n=== Training Setup ===\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "cer_scores = []\n",
    "wer_scores = []\n",
    "\n",
    "# ============================================\n",
    "# PART 9: Training Loop\n",
    "# ============================================\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs} - Training\")\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "        for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels.to(device))\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # Decode predictions and references\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "            predictions = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            references = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    validation_losses.append(avg_val_loss)\n",
    "\n",
    "    # Calculate CER and WER\n",
    "    cer_score = cer(all_references, all_predictions)\n",
    "    wer_score = wer(all_references, all_predictions)\n",
    "    cer_scores.append(cer_score)\n",
    "    wer_scores.append(wer_score)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 10: Plot Training Results\n",
    "# ============================================\n",
    "print(\"\\n=== Plotting Results ===\")\n",
    "epochs_range = range(1, epochs + 1)\n",
    "\n",
    "# Training and Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, training_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(epochs_range, validation_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# CER and WER\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_range, cer_scores, label=\"CER\", marker='o')\n",
    "plt.plot(epochs_range, wer_scores, label=\"WER\", marker='s')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"CER and WER over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# PART 11: Save Model\n",
    "# ============================================\n",
    "print(\"\\n=== Saving Model ===\")\n",
    "model.save_pretrained(\"khmer_text_recognition_model_v3\")\n",
    "processor.save_pretrained(\"khmer_text_recognition_processor_v3\")\n",
    "print(\"Model and processor saved successfully!\")\n",
    "\n",
    "# ============================================\n",
    "# PART 12: Test the Model\n",
    "# ============================================\n",
    "print(\"\\n=== Testing Model ===\")\n",
    "# Load saved model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"khmer_text_recognition_processor_v3\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"khmer_text_recognition_model_v3\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_preds, test_refs = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(pixel_values, max_new_tokens=128)\n",
    "        decoded_preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        test_preds.extend(decoded_preds)\n",
    "        test_refs.extend(decoded_labels)\n",
    "\n",
    "# Calculate CER and WER\n",
    "test_cer = cer(test_refs, test_preds)\n",
    "test_wer = wer(test_refs, test_preds)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "for i, (pred, ref) in enumerate(zip(test_preds[:10], test_refs[:10])):\n",
    "    print(f\"\\n{i+1}.\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Display overall metrics\n",
    "print(\"\\n=== Overall Test Metrics ===\")\n",
    "print(f\"Character Error Rate (CER): {test_cer:.4f}\")\n",
    "print(f\"Word Error Rate (WER): {test_wer:.4f}\")\n",
    "print(\"\\n=== Training Complete ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
